{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7a2e345-8e90-4ac5-a954-c2c024cda590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817efaed-c67e-48d0-a1f0-49b4638fb098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input DataFrame type: <class 'pandas.core.frame.DataFrame'>\n",
      "Total instances in original dataset: 690\n",
      "Instances held for extrapolation: 11\n",
      "Instances used for training/validation: 679\n",
      "\n",
      "Target Statistics (final_expandEvts):\n",
      "Mean (censored=0): 53227800.02\n",
      "Std Dev (censored=0): 174281183.67\n",
      "Selected features (70): ['num_stackdepth3_logs', 'evts_1', 'expandEvts_1', 'pruneBacktrackEvts_1', 'backtrackEvts_1', 'strengthenEvts_1', 'maxStackDepth_1', 'evts_2', 'expandEvts_2', 'pruneBacktrackEvts_2', 'backtrackEvts_2', 'strengthenEvts_2', 'maxStackDepth_2', 'evts_3', 'expandEvts_3', 'pruneBacktrackEvts_3', 'backtrackEvts_3', 'strengthenEvts_3', 'maxStackDepth_3', 'avg_evts', 'max_evts', 'avg_expandEvts', 'max_expandEvts', 'avg_pruneBacktrackEvts', 'max_pruneBacktrackEvts', 'evts_4', 'expandEvts_4', 'pruneBacktrackEvts_4', 'backtrackEvts_4', 'strengthenEvts_4', 'maxStackDepth_4', 'evts_5', 'expandEvts_5', 'pruneBacktrackEvts_5', 'backtrackEvts_5', 'strengthenEvts_5', 'maxStackDepth_5', 'n', 'k', 'total_sum', 'variance', 'skewness', 'max_num', 'min_num', 'avg_subset_sum', 'max_to_avg_ratio', 'range_to_avg_ratio', 'coef_of_variation', 'expandEvts_ratio_1', 'pruneBacktrackEvts_ratio_1', 'expandEvts_ratio_2', 'pruneBacktrackEvts_ratio_2', 'expandEvts_ratio_3', 'pruneBacktrackEvts_ratio_3', 'expandEvts_ratio_4', 'pruneBacktrackEvts_ratio_4', 'expandEvts_ratio_5', 'pruneBacktrackEvts_ratio_5', 'diff_evts_2', 'diff_expandEvts_2', 'diff_pruneBacktrackEvts_2', 'diff_evts_3', 'diff_expandEvts_3', 'diff_pruneBacktrackEvts_3', 'diff_evts_4', 'diff_expandEvts_4', 'diff_pruneBacktrackEvts_4', 'diff_evts_5', 'diff_expandEvts_5', 'diff_pruneBacktrackEvts_5']\n",
      "\n",
      "Starting GridSearchCV for 'censored' data...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates various regression metrics including RMSE, Normalized RMSE, SMAPE, and SMDApe.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): The actual target values.\n",
    "        y_pred (np.array): The predicted target values.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Avoid division by zero by adding a small epsilon to the denominator\n",
    "        epsilon = 1e-10\n",
    "\n",
    "        # Calculate Root Mean Squared Error (RMSE)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "        # Calculate Normalized Root Mean Squared Error (Normalized RMSE) as a percentage\n",
    "        # using the mean of the actual values. This is a key change.\n",
    "        norm_rmse = (rmse / (np.mean(y_true) + epsilon)) * 100\n",
    "\n",
    "        # Calculate Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "        smape = np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + epsilon)) * 100\n",
    "\n",
    "        # Calculate Symmetric Mean Directional Absolute Percentage Error (SMDAPE)\n",
    "        # This is similar to SMAPE but with a directional component\n",
    "        smdape = np.mean((y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + epsilon)) * 100\n",
    "\n",
    "        return {\n",
    "            'rmse': rmse,\n",
    "            'norm_rmse': norm_rmse,\n",
    "            'smape': smape,\n",
    "            'smdape': smdape\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_metrics: {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_predictions(y_actual, y_predicted, title):\n",
    "    \"\"\"\n",
    "    Plots the predicted values against the actual values to visualize model performance.\n",
    "    \n",
    "    Args:\n",
    "        y_actual (pd.Series or np.array): The actual target values.\n",
    "        y_predicted (np.array): The predicted values from the model.\n",
    "        title (str): The title for the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Create a scatter plot of actual vs predicted values\n",
    "    plt.scatter(y_actual, y_predicted, alpha=0.5, label='Predicted values')\n",
    "    \n",
    "    # Get the min and max values for the ideal line\n",
    "    min_val = min(y_actual.min(), y_predicted.min())\n",
    "    max_val = max(y_actual.max(), y_predicted.max())\n",
    "    \n",
    "    # Plot the ideal prediction line (y=x)\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal Prediction')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def train_lightgbm_model(df, include_ratios=False):\n",
    "    \"\"\"\n",
    "    Trains a LightGBM model on the given dataset and evaluates its performance.\n",
    "    \n",
    "    This function now includes hyperparameter tuning using GridSearchCV to find the\n",
    "    optimal parameters for the 'censored' model.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input data frame.\n",
    "        include_ratios (bool): Whether to include ratio features in the training.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary of evaluation results, feature importance, and the\n",
    "              best hyperparameters found by GridSearchCV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input\n",
    "        print(f\"Input DataFrame type: {type(df)}\")\n",
    "        if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "            print(\"Input DataFrame is not valid.\")\n",
    "            return None\n",
    "        \n",
    "        # Define the extrapolation filenames based on your previous notebooks\n",
    "        extrapolation_filenames = [\n",
    "            'n27k3_v1.txt', 'n27k3_v2.txt', 'n27k3_v3.txt', 'n27k3_v4.txt',\n",
    "            'n28k3_v1.txt', 'n28k3_v2.txt', 'n28k3_v3.txt', 'n28k3_v4.txt',\n",
    "            'n28k3_v5.txt', 'n29k3_v1.txt', 'n29k3_v2.txt'\n",
    "        ]\n",
    "\n",
    "        # Separate extrapolation data based on filename\n",
    "        df_extrapolation = df[df['filename'].isin(extrapolation_filenames)].copy()\n",
    "        df_train_val = df[~df['filename'].isin(extrapolation_filenames)].copy()\n",
    "\n",
    "        # Separate the censored data for a 'no timeout' model\n",
    "        df_no_timeout = df_train_val[df_train_val['censored'] == 0].copy()\n",
    "        \n",
    "        print(f\"Total instances in original dataset: {len(df)}\")\n",
    "        print(f\"Instances held for extrapolation: {len(df_extrapolation)}\")\n",
    "        print(f\"Instances used for training/validation: {len(df_train_val)}\")\n",
    "        print(\"\\nTarget Statistics (final_expandEvts):\")\n",
    "        print(f\"Mean (censored=0): {df_no_timeout['final_expandEvts'].mean():.2f}\")\n",
    "        print(f\"Std Dev (censored=0): {df_no_timeout['final_expandEvts'].std():.2f}\")\n",
    "\n",
    "        # Dynamically select features based on what's available in the dataframe\n",
    "        exclude_cols = ['filename', 'final_expandEvts', 'stop_iter', 'final_maxStackDepth']\n",
    "        if not include_ratios:\n",
    "            exclude_cols.extend([f'expandEvts_ratio_{i}' for i in range(1, 6)])\n",
    "            exclude_cols.extend([f'pruneBacktrackEvts_ratio_{i}' for i in range(1, 6)])\n",
    "        features = [col for col in df.columns if col not in exclude_cols and col != 'censored']\n",
    "\n",
    "        print(f\"Selected features ({len(features)}): {features}\")\n",
    "        \n",
    "        target = 'final_expandEvts'\n",
    "\n",
    "        # Separate target variable from features for all three datasets\n",
    "        X_censored = df_no_timeout[features]\n",
    "        y_censored = df_no_timeout[target]\n",
    "        X_all = df_train_val[features]\n",
    "        y_all = df_train_val[target]\n",
    "        X_extrapolation = df_extrapolation[features]\n",
    "        y_extrapolation = df_extrapolation[target]\n",
    "\n",
    "        # Handle potential zero values in target to avoid log-transform issues\n",
    "        y_censored = y_censored + 1\n",
    "        y_all = y_all + 1\n",
    "        y_extrapolation = y_extrapolation + 1\n",
    "        \n",
    "        # --- Start of Hyperparameter Tuning Section ---\n",
    "        \n",
    "        # Define the parameter grid to search over\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.05, 0.1, 0.15],\n",
    "            'num_leaves': [31, 50, 70],\n",
    "            'reg_alpha': [0, 0.1]\n",
    "        }\n",
    "        \n",
    "        # Create a base LightGBM model with warnings suppressed\n",
    "        lgbm = lgb.LGBMRegressor(random_state=42, verbose=-1)\n",
    "        \n",
    "        # Set up GridSearchCV with 5-fold cross-validation\n",
    "        # We use 'neg_root_mean_squared_error' as the scoring metric.\n",
    "        # The best model will have the lowest RMSE (highest negative RMSE).\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=lgbm,\n",
    "            param_grid=param_grid,\n",
    "            cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            verbose=1,\n",
    "            n_jobs=-1  # Use all available CPU cores\n",
    "        )\n",
    "        \n",
    "        print(\"\\nStarting GridSearchCV for 'censored' data...\")\n",
    "        # Fit the grid search to find the best model for the censored data\n",
    "        # We are training on the log-transformed target variable\n",
    "        grid_search.fit(X_censored, np.log1p(y_censored))\n",
    "        \n",
    "        # Get the best parameters and best model from the grid search\n",
    "        best_params_censored = grid_search.best_params_\n",
    "        lgbm_censored = grid_search.best_estimator_\n",
    "        \n",
    "        print(\"\\nGridSearchCV finished.\")\n",
    "        print(f\"Best parameters for censored model: {best_params_censored}\")\n",
    "        \n",
    "        # --- End of Hyperparameter Tuning Section ---\n",
    "        \n",
    "        # Train a model on censored=0 (non-timeout) data using the best estimator\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_censored, y_censored, test_size=0.2, random_state=42)\n",
    "        # lgbm_censored is already the best estimator, so we just use it to predict on the test set\n",
    "        y_pred_censored_test_log = lgbm_censored.predict(X_test)\n",
    "        y_pred_censored_test = np.expm1(y_pred_censored_test_log)\n",
    "        \n",
    "        # Evaluate censored model on test set\n",
    "        metrics_censored_test = calculate_metrics(y_test, y_pred_censored_test)\n",
    "        \n",
    "        # Cross-validation for censored model (this is now redundant due to GridSearchCV but kept for a full CV score)\n",
    "        cv_rmse_censored = -grid_search.best_score_\n",
    "        cv_norm_rmse_censored = cv_rmse_censored / np.mean(y_censored) * 100\n",
    "        \n",
    "        # Train a model on all data (including timeouts) - no tuning here to simplify\n",
    "        lgbm_all = lgb.LGBMRegressor(random_state=42, verbose=-1)\n",
    "        lgbm_all.fit(X_all, np.log1p(y_all))\n",
    "        y_pred_all = np.expm1(lgbm_all.predict(X_all))\n",
    "        \n",
    "        # Evaluate all data model\n",
    "        metrics_all = calculate_metrics(y_all, y_pred_all)\n",
    "        \n",
    "        # Cross-validation for all data model\n",
    "        cv_scores_all = cross_val_score(lgbm_all, X_all, np.log1p(y_all), cv=KFold(n_splits=5, shuffle=True, random_state=42), scoring='neg_root_mean_squared_error')\n",
    "        cv_rmse_all = -np.mean(cv_scores_all)\n",
    "        cv_norm_rmse_all = cv_rmse_all / np.mean(y_all) * 100\n",
    "\n",
    "        # Predict on extrapolation data using the tuned censored model\n",
    "        y_pred_extrapolation_log = lgbm_censored.predict(X_extrapolation)\n",
    "        y_pred_extrapolation = np.expm1(y_pred_extrapolation_log)\n",
    "        metrics_extrapolation = calculate_metrics(y_extrapolation, y_pred_extrapolation)\n",
    "\n",
    "        # Feature importance from the 'all data' model\n",
    "        importance = pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': lgbm_all.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Store results in a dictionary\n",
    "        results = {\n",
    "            'best_params_censored': best_params_censored, # Store the best parameters\n",
    "            'rmse_censored_test': metrics_censored_test['rmse'],\n",
    "            'norm_rmse_censored_test': metrics_censored_test['norm_rmse'],\n",
    "            'smape_censored_test': metrics_censored_test['smape'],\n",
    "            'smdape_censored_test': metrics_censored_test['smdape'],\n",
    "            'cv_norm_rmse_censored': cv_norm_rmse_censored,\n",
    "            'rmse_all': metrics_all['rmse'],\n",
    "            'norm_rmse_all': metrics_all['norm_rmse'],\n",
    "            'smape_all': metrics_all['smape'],\n",
    "            'smdape_all': metrics_all['smdape'],\n",
    "            'cv_norm_rmse_all': cv_norm_rmse_all,\n",
    "            'feature_importance': importance,\n",
    "            'rmse_extrapolation': metrics_extrapolation['rmse'],\n",
    "            'norm_rmse_extrapolation': metrics_extrapolation['norm_rmse'],\n",
    "            'smape_extrapolation': metrics_extrapolation['smape'],\n",
    "            'smdape_extrapolation': metrics_extrapolation['smdape'],\n",
    "            'y_test': y_test,\n",
    "            'y_pred_censored_test': y_pred_censored_test,\n",
    "            'y_extrapolation': y_extrapolation,\n",
    "            'y_pred_extrapolation': y_pred_extrapolation\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_lightgbm_model: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main execution block ---\n",
    "# NOTE: This block assumes 'structured_data.xlsx' is available in the same directory.\n",
    "# This code is for demonstration and requires the data file to run.\n",
    "try:\n",
    "    df = pd.read_excel(\"structured_data.xlsx\")\n",
    "    results = train_lightgbm_model(df, include_ratios=True)\n",
    "    print(results)\n",
    "\n",
    "    if results:\n",
    "        # Plotting for the censored (non-timeout) test data\n",
    "        plot_predictions(results['y_test'], results['y_pred_censored_test'], 'Predicted vs. Actual Values (Non-Timeout Test Data)')\n",
    "        \n",
    "        # Plotting for the extrapolation data\n",
    "        plot_predictions(results['y_extrapolation'], results['y_pred_extrapolation'], 'Predicted vs. Actual Values (Extrapolation Data)')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: structured_data.xlsx not found. Please provide the data file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-manipulation-class",
   "language": "python",
   "name": "data-manipulation-class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
