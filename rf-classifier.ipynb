{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369d2521-2ba0-41d8-95a1-5e437e74aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c485df17-44f3-4a9f-b63e-72c6f651b6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input DataFrame type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input columns: ['num_stackdepth3_logs', 'evts_1', 'expandEvts_1', 'pruneBacktrackEvts_1', 'backtrackEvts_1', 'strengthenEvts_1', 'maxStackDepth_1', 'evts_2', 'expandEvts_2', 'pruneBacktrackEvts_2', 'backtrackEvts_2', 'strengthenEvts_2', 'maxStackDepth_2', 'evts_3', 'expandEvts_3', 'pruneBacktrackEvts_3', 'backtrackEvts_3', 'strengthenEvts_3', 'maxStackDepth_3', 'censored', 'final_expandEvts', 'final_maxStackDepth', 'stop_iter', 'avg_evts', 'max_evts', 'avg_expandEvts', 'max_expandEvts', 'avg_pruneBacktrackEvts', 'max_pruneBacktrackEvts', 'evts_4', 'expandEvts_4', 'pruneBacktrackEvts_4', 'backtrackEvts_4', 'strengthenEvts_4', 'maxStackDepth_4', 'evts_5', 'expandEvts_5', 'pruneBacktrackEvts_5', 'backtrackEvts_5', 'strengthenEvts_5', 'maxStackDepth_5', 'n', 'k', 'total_sum', 'variance', 'skewness', 'max_num', 'min_num', 'avg_subset_sum', 'max_to_avg_ratio', 'range_to_avg_ratio', 'coef_of_variation', 'expandEvts_ratio_1', 'pruneBacktrackEvts_ratio_1', 'expandEvts_ratio_2', 'pruneBacktrackEvts_ratio_2', 'expandEvts_ratio_3', 'pruneBacktrackEvts_ratio_3', 'expandEvts_ratio_4', 'pruneBacktrackEvts_ratio_4', 'expandEvts_ratio_5', 'pruneBacktrackEvts_ratio_5', 'diff_evts_2', 'diff_expandEvts_2', 'diff_pruneBacktrackEvts_2', 'diff_evts_3', 'diff_expandEvts_3', 'diff_pruneBacktrackEvts_3', 'diff_evts_4', 'diff_expandEvts_4', 'diff_pruneBacktrackEvts_4', 'diff_evts_5', 'diff_expandEvts_5', 'diff_pruneBacktrackEvts_5']\n",
      "Selected features (60): ['num_stackdepth3_logs', 'evts_1', 'expandEvts_1', 'pruneBacktrackEvts_1', 'backtrackEvts_1', 'strengthenEvts_1', 'maxStackDepth_1', 'evts_2', 'expandEvts_2', 'pruneBacktrackEvts_2', 'backtrackEvts_2', 'strengthenEvts_2', 'maxStackDepth_2', 'evts_3', 'expandEvts_3', 'pruneBacktrackEvts_3', 'backtrackEvts_3', 'strengthenEvts_3', 'maxStackDepth_3', 'avg_evts', 'max_evts', 'avg_expandEvts', 'max_expandEvts', 'avg_pruneBacktrackEvts', 'max_pruneBacktrackEvts', 'evts_4', 'expandEvts_4', 'pruneBacktrackEvts_4', 'backtrackEvts_4', 'strengthenEvts_4', 'maxStackDepth_4', 'evts_5', 'expandEvts_5', 'pruneBacktrackEvts_5', 'backtrackEvts_5', 'strengthenEvts_5', 'maxStackDepth_5', 'n', 'k', 'total_sum', 'variance', 'skewness', 'max_num', 'min_num', 'avg_subset_sum', 'max_to_avg_ratio', 'range_to_avg_ratio', 'coef_of_variation', 'diff_evts_2', 'diff_expandEvts_2', 'diff_pruneBacktrackEvts_2', 'diff_evts_3', 'diff_expandEvts_3', 'diff_pruneBacktrackEvts_3', 'diff_evts_4', 'diff_expandEvts_4', 'diff_pruneBacktrackEvts_4', 'diff_evts_5', 'diff_expandEvts_5', 'diff_pruneBacktrackEvts_5']\n",
      "\n",
      "Class Distribution (censored):\n",
      "Completed (censored=0): 299 instances\n",
      "Timeout (censored=1): 391 instances\n",
      "\n",
      "Training Random Forest Classifier on all data...\n",
      "Test Set Accuracy: 0.9928\n",
      "Test Set Precision (timeout): 0.9873\n",
      "Test Set Recall (timeout): 1.0000\n",
      "Test Set F1-Score (timeout): 0.9936\n",
      "\n",
      "Confusion Matrix:\n",
      "[[True Neg (Completed)=59, False Pos=1]\n",
      "[[False Neg=0, True Pos (Timeout)=78]\n",
      "\n",
      "Performing cross-validation...\n",
      "Cross-Validation Accuracy: 0.9841 (+/- 0.0382)\n",
      "\n",
      "Feature Importance (Top 10):\n",
      "                      feature  importance\n",
      "23     avg_pruneBacktrackEvts    0.132055\n",
      "20                   max_evts    0.122311\n",
      "19                   avg_evts    0.093069\n",
      "24     max_pruneBacktrackEvts    0.092482\n",
      "21             avg_expandEvts    0.081413\n",
      "7                      evts_2    0.070742\n",
      "22             max_expandEvts    0.051920\n",
      "48                diff_evts_2    0.049771\n",
      "49          diff_expandEvts_2    0.049211\n",
      "50  diff_pruneBacktrackEvts_2    0.040877\n",
      "Saved test set predictions to plots/predictions_classifier_test.csv\n",
      "Saved feature importance to plots/feature_importance_classifier.csv\n",
      "Saved all-data predictions to plots/predictions_classifier_all.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9927536231884058,\n",
       " 'precision_timeout': 0.9873417721518988,\n",
       " 'recall_timeout': 1.0,\n",
       " 'f1_timeout': 0.9936305732484076,\n",
       " 'confusion_matrix': array([[59,  1],\n",
       "        [ 0, 78]]),\n",
       " 'cv_accuracy': 0.9840579710144928,\n",
       " 'cv_accuracy_std': 0.019117255011989735,\n",
       " 'feature_importance':                       feature  importance\n",
       " 23     avg_pruneBacktrackEvts    0.132055\n",
       " 20                   max_evts    0.122311\n",
       " 19                   avg_evts    0.093069\n",
       " 24     max_pruneBacktrackEvts    0.092482\n",
       " 21             avg_expandEvts    0.081413\n",
       " 7                      evts_2    0.070742\n",
       " 22             max_expandEvts    0.051920\n",
       " 48                diff_evts_2    0.049771\n",
       " 49          diff_expandEvts_2    0.049211\n",
       " 50  diff_pruneBacktrackEvts_2    0.040877\n",
       " 9        pruneBacktrackEvts_2    0.039152\n",
       " 37                          n    0.033206\n",
       " 8                expandEvts_2    0.030215\n",
       " 39                  total_sum    0.024036\n",
       " 14               expandEvts_3    0.017531\n",
       " 0        num_stackdepth3_logs    0.015336\n",
       " 13                     evts_3    0.008854\n",
       " 12            maxStackDepth_2    0.008759\n",
       " 16            backtrackEvts_3    0.007866\n",
       " 52          diff_expandEvts_3    0.007533\n",
       " 18            maxStackDepth_3    0.007298\n",
       " 53  diff_pruneBacktrackEvts_3    0.002760\n",
       " 59  diff_pruneBacktrackEvts_5    0.002407\n",
       " 56  diff_pruneBacktrackEvts_4    0.002040\n",
       " 55          diff_expandEvts_4    0.001630\n",
       " 54                diff_evts_4    0.001438\n",
       " 57                diff_evts_5    0.000839\n",
       " 32               expandEvts_5    0.000804\n",
       " 31                     evts_5    0.000599\n",
       " 58          diff_expandEvts_5    0.000509\n",
       " 15       pruneBacktrackEvts_3    0.000477\n",
       " 25                     evts_4    0.000359\n",
       " 10            backtrackEvts_2    0.000302\n",
       " 45           max_to_avg_ratio    0.000245\n",
       " 51                diff_evts_3    0.000213\n",
       " 46         range_to_avg_ratio    0.000211\n",
       " 26               expandEvts_4    0.000208\n",
       " 36            maxStackDepth_5    0.000204\n",
       " 27       pruneBacktrackEvts_4    0.000194\n",
       " 35           strengthenEvts_5    0.000178\n",
       " 40                   variance    0.000138\n",
       " 47          coef_of_variation    0.000137\n",
       " 41                   skewness    0.000118\n",
       " 28            backtrackEvts_4    0.000117\n",
       " 44             avg_subset_sum    0.000077\n",
       " 43                    min_num    0.000074\n",
       " 30            maxStackDepth_4    0.000055\n",
       " 34            backtrackEvts_5    0.000030\n",
       " 38                          k    0.000003\n",
       " 42                    max_num    0.000000\n",
       " 6             maxStackDepth_1    0.000000\n",
       " 5            strengthenEvts_1    0.000000\n",
       " 4             backtrackEvts_1    0.000000\n",
       " 3        pruneBacktrackEvts_1    0.000000\n",
       " 2                expandEvts_1    0.000000\n",
       " 11           strengthenEvts_2    0.000000\n",
       " 33       pruneBacktrackEvts_5    0.000000\n",
       " 1                      evts_1    0.000000\n",
       " 29           strengthenEvts_4    0.000000\n",
       " 17           strengthenEvts_3    0.000000,\n",
       " 'y_test': 532    1\n",
       " 495    1\n",
       " 1      0\n",
       " 210    0\n",
       " 250    0\n",
       "       ..\n",
       " 362    1\n",
       " 164    1\n",
       " 87     0\n",
       " 391    1\n",
       " 689    0\n",
       " Name: censored, Length: 138, dtype: int64,\n",
       " 'y_pred': array([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "        0, 1, 1, 0, 1, 0]),\n",
       " 'y_test_all': 0      0\n",
       " 1      0\n",
       " 2      0\n",
       " 3      0\n",
       " 4      0\n",
       "       ..\n",
       " 685    0\n",
       " 686    0\n",
       " 687    0\n",
       " 688    0\n",
       " 689    0\n",
       " Name: censored, Length: 690, dtype: int64,\n",
       " 'y_pred_all': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_random_forest_classifier(df, include_ratios=False, output_dir='plots', cv_folds=5):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest classification model on all data to predict whether an instance is timeout (censored=1)\n",
    "    or completed (censored=0). Evaluates performance using accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): Preprocessed DataFrame with solver features and censored column.\n",
    "    - include_ratios (bool): If True, includes ratio features; if False, excludes them (default: False).\n",
    "    - output_dir (str): Directory to save predictions and feature importance (default: 'plots').\n",
    "    - cv_folds (int): Number of cross-validation folds (default: 5).\n",
    "\n",
    "    Returns:\n",
    "    - dict: Contains classification metrics, cross-validation accuracy, feature importance, and predictions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(f\"Input 'df' must be a pandas DataFrame, got {type(df)}\")\n",
    "        print(f\"Input DataFrame type: {type(df)}\")\n",
    "        # print(f\"Input columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Feature selection\n",
    "        exclude_cols = ['filename', 'final_expandEvts', 'stop_iter', 'final_maxStackDepth', 'censored']\n",
    "        if not include_ratios:\n",
    "            exclude_cols.extend([f'expandEvts_ratio_{i}' for i in range(1, 6)])\n",
    "            exclude_cols.extend([f'pruneBacktrackEvts_ratio_{i}' for i in range(1, 6)])\n",
    "        features = [col for col in df.columns if col not in exclude_cols]\n",
    "        print(f\"Selected features ({len(features)}): {features}\")\n",
    "        \n",
    "        # Print class distribution\n",
    "        print(\"\\nClass Distribution (censored):\")\n",
    "        class_counts = df['censored'].value_counts()\n",
    "        print(f\"Completed (censored=0): {class_counts.get(0, 0)} instances\")\n",
    "        print(f\"Timeout (censored=1): {class_counts.get(1, 0)} instances\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X = df[features]\n",
    "        y = df['censored'].astype(int)  # Ensure binary labels (0 or 1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        # Train Random Forest Classifier\n",
    "        print(\"\\nTraining Random Forest Classifier on all data...\")\n",
    "        rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf_classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = rf_classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "        recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "        f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "        \n",
    "        print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Test Set Precision (timeout): {precision:.4f}\")\n",
    "        print(f\"Test Set Recall (timeout): {recall:.4f}\")\n",
    "        print(f\"Test Set F1-Score (timeout): {f1:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(f\"[[True Neg (Completed)={cm[0,0]}, False Pos={cm[0,1]}]\")\n",
    "        print(f\"[[False Neg={cm[1,0]}, True Pos (Timeout)={cm[1,1]}]\")\n",
    "        \n",
    "        # Cross-validation\n",
    "        print(\"\\nPerforming cross-validation...\")\n",
    "        cv_scores = cross_val_score(rf_classifier, X, y, cv=cv_folds, scoring='accuracy')\n",
    "        cv_accuracy = cv_scores.mean()\n",
    "        print(f\"Cross-Validation Accuracy: {cv_accuracy:.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        # Feature importance\n",
    "        importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': rf_classifier.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(\"\\nFeature Importance (Top 10):\")\n",
    "        print(importance.head(10))\n",
    "        \n",
    "        # Save predictions and feature importance\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        pd.DataFrame({\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred\n",
    "        }).to_csv(os.path.join(output_dir, 'predictions_classifier_test.csv'), index=False)\n",
    "        importance.to_csv(os.path.join(output_dir, 'feature_importance_classifier.csv'), index=False)\n",
    "        print(f\"Saved test set predictions to {output_dir}/predictions_classifier_test.csv\")\n",
    "        print(f\"Saved feature importance to {output_dir}/feature_importance_classifier.csv\")\n",
    "        \n",
    "        # Predict on all data for completeness\n",
    "        y_pred_all = rf_classifier.predict(X)\n",
    "        pd.DataFrame({\n",
    "            'y_test': y,\n",
    "            'y_pred': y_pred_all\n",
    "        }).to_csv(os.path.join(output_dir, 'predictions_classifier_all.csv'), index=False)\n",
    "        print(f\"Saved all-data predictions to {output_dir}/predictions_classifier_all.csv\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision_timeout': precision,\n",
    "            'recall_timeout': recall,\n",
    "            'f1_timeout': f1,\n",
    "            'confusion_matrix': cm,\n",
    "            'cv_accuracy': cv_accuracy,\n",
    "            'cv_accuracy_std': cv_scores.std(),\n",
    "            'feature_importance': importance,\n",
    "            # 'y_test': y_test,\n",
    "            # 'y_pred': y_pred,\n",
    "            # 'y_test_all': y,\n",
    "            # 'y_pred_all': y_pred_all\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_random_forest_classifier: {e}\")\n",
    "        return None\n",
    "\n",
    "df = pd.read_excel(\"structured_data.xlsx\")\n",
    "train_random_forest_classifier(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-manipulation-class",
   "language": "python",
   "name": "data-manipulation-class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
