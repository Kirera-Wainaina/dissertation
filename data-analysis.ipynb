{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ac526d-81a4-4ce9-9082-121a8202f5de",
   "metadata": {},
   "source": [
    "# Transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d943dd-1f78-49ba-8b57-21976aa1cc2f",
   "metadata": {},
   "source": [
    "## Objective: \n",
    "Create a pipeline to transform the log data into a dataframe we can use for predictive modelling.\n",
    "\n",
    "## Table format: Features\n",
    "\n",
    "### Input Features\n",
    "\n",
    "- $ n $: Number of elements (e.g., 16, 31).\n",
    "- $ k $: Number of partitions (e.g., 5, 4).\n",
    "- Total sum: $ \\sum S $ (requires input numbers).\n",
    "- Variance: $ \\text{var}(S) $.\n",
    "- Skewness: Distribution shape\n",
    "- Max/min number.\n",
    "- Average subset sum: $ \\text{total sum} / k $.\n",
    "\n",
    "### Solver Features (First $ k $ Logs at stackDepth=3):\n",
    "\n",
    "**For each log (up to $ k $):**\n",
    "\n",
    "- evts: Events at stackDepth=3.\n",
    "- expandEvts: Expansions.\n",
    "- pruneBacktrackEvts: Pruning backtracks.\n",
    "- backtrackEvts: Non-pruning backtracks.\n",
    "- strengthenEvts: Constraint tightenings.\n",
    "- maxStackDepth: Maximum depth reached.\n",
    "- Subset sums: Sum of numbers assigned to each subset based on path (requires input numbers).\n",
    "- Subset sum variance: Variance of subset sums.\n",
    "- Aggregated: Average or max evts, expandEvts, pruneBacktrackEvts across the $ k $ logs.\n",
    "- num_stackdepth3_logs: Number of stackDepth=3 logs (proxy for search difficulty).\n",
    "\n",
    "### Termination/Timeout Features\n",
    "- expandEvts (target variable).\n",
    "- Censored flag: 1 for timeouts, 0 for completions.\n",
    "- Objective value: maxsum - minsum (if available, e.g., 2 for $ n=10, k=3 $)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4147a1e7-32ff-4a08-8344-b1b56a408eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ab28a-c06c-4ddc-b433-f0edbfdd13c6",
   "metadata": {},
   "source": [
    "# 1. Create a dataframe with solver features\n",
    "The solver features are as listed above. The index will be the file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59262034-d4ef-465c-b1dc-23d5495289eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features for 620 instances\n",
      "DataFrame shape: (620, 42)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_ml_features(jsonl_path):\n",
    "    \"\"\"\n",
    "    Extract ML features from the ml_features.jsonl file.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_path: Path to the ml_features.jsonl file\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the extracted features\n",
    "    \"\"\"\n",
    "    # List to store data for each instance\n",
    "    data_list = []\n",
    "    \n",
    "    # Open and process the JSONL file\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                # Parse the JSON line\n",
    "                line_data = json.loads(line.strip())\n",
    "                \n",
    "                # Each line contains a single key (filename) with an array of log entries\n",
    "                for filename, logs in line_data.items():\n",
    "                    if not logs:  # Skip if no logs\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract n and k from filename using regex\n",
    "                    match = re.search(r'n(\\d+)k(\\d+)', filename)\n",
    "                    if match:\n",
    "                        n = int(match.group(1))  # Number of values\n",
    "                        k = int(match.group(2))  # Number of partitions\n",
    "                    else:\n",
    "                        # If pattern doesn't match, try to infer from the logs\n",
    "                        k = max(3, min(5, len(logs)))\n",
    "                        n = 0  # Unknown\n",
    "                    \n",
    "                    # Initialize feature dictionary\n",
    "                    features = {\n",
    "                        'filename': filename,\n",
    "                        'n': n,\n",
    "                        'k': k,\n",
    "                        'num_stackdepth3_logs': 0\n",
    "                    }\n",
    "                    \n",
    "                    # Extract individual log features (up to k logs)\n",
    "                    for i in range(min(k, len(logs))):\n",
    "                        log = logs[i]\n",
    "                        \n",
    "                        if log.get('stackDepth', 0) == 3:\n",
    "                            features['num_stackdepth3_logs'] += 1\n",
    "                            \n",
    "                        # Extract all numeric features from this log\n",
    "                        for field in ['evts', 'expandEvts', 'pruneBacktrackEvts', \n",
    "                                     'backtrackEvts', 'strengthenEvts', 'maxStackDepth']:\n",
    "                            if field in log:\n",
    "                                features[f'{field}_{i+1}'] = log[field]\n",
    "                    \n",
    "                    # Find the termination or timeout event (should be the last log)\n",
    "                    last_log = logs[-1]\n",
    "                    is_timeout = last_log.get('event') == 'TIMEOUT'\n",
    "                    \n",
    "                    # Add target variables\n",
    "                    features['censored'] = 1 if is_timeout else 0\n",
    "                    features['final_expandEvts'] = last_log.get('expandEvts', 0)\n",
    "                    features['final_maxStackDepth'] = last_log.get('maxStackDepth', 0)\n",
    "                    \n",
    "                    # Calculate aggregated features\n",
    "                    for field in ['evts', 'expandEvts', 'pruneBacktrackEvts']:\n",
    "                        values = [log.get(field, 0) for log in logs[:k] if field in log]\n",
    "                        if values:\n",
    "                            features[f'avg_{field}'] = sum(values) / len(values)\n",
    "                            features[f'max_{field}'] = max(values)\n",
    "                    \n",
    "                    # Add to data list\n",
    "                    data_list.append(features)\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON line: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df = df.set_index(\"filename\")\n",
    "    \n",
    "    # For demonstration, print the shape and first few rows\n",
    "    print(f\"Extracted features for {len(df)} instances\")\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "solver_features_df = extract_ml_features(\"ml_features.jsonl\")\n",
    "# df.to_csv(\"ml_features.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70d33b8f-50c2-493a-be73-dc5239756cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>k</th>\n",
       "      <th>num_stackdepth3_logs</th>\n",
       "      <th>evts_1</th>\n",
       "      <th>expandEvts_1</th>\n",
       "      <th>pruneBacktrackEvts_1</th>\n",
       "      <th>backtrackEvts_1</th>\n",
       "      <th>strengthenEvts_1</th>\n",
       "      <th>maxStackDepth_1</th>\n",
       "      <th>evts_2</th>\n",
       "      <th>...</th>\n",
       "      <th>pruneBacktrackEvts_4</th>\n",
       "      <th>backtrackEvts_4</th>\n",
       "      <th>strengthenEvts_4</th>\n",
       "      <th>maxStackDepth_4</th>\n",
       "      <th>evts_5</th>\n",
       "      <th>expandEvts_5</th>\n",
       "      <th>pruneBacktrackEvts_5</th>\n",
       "      <th>backtrackEvts_5</th>\n",
       "      <th>strengthenEvts_5</th>\n",
       "      <th>maxStackDepth_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n10k3_v1.txt</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10k3_v2.txt</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10k3_v3.txt</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10k3_v4.txt</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>106</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10k3_v5.txt</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v1.txt</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2318009416</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v2.txt</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2511054050</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v3.txt</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2402044766</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v4.txt</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2319516173</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v5.txt</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2299941979</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               n  k  num_stackdepth3_logs  evts_1  expandEvts_1  \\\n",
       "filename                                                          \n",
       "n10k3_v1.txt  10  3                     3       4             4   \n",
       "n10k3_v2.txt  10  3                     3       4             4   \n",
       "n10k3_v3.txt  10  3                     3       4             4   \n",
       "n10k3_v4.txt  10  3                     3       4             4   \n",
       "n10k3_v5.txt  10  3                     3       4             4   \n",
       "...           .. ..                   ...     ...           ...   \n",
       "n49k4_v1.txt  49  4                     1       4             4   \n",
       "n49k4_v2.txt  49  4                     1       4             4   \n",
       "n49k4_v3.txt  49  4                     1       4             4   \n",
       "n49k4_v4.txt  49  4                     1       4             4   \n",
       "n49k4_v5.txt  49  4                     1       4             4   \n",
       "\n",
       "              pruneBacktrackEvts_1  backtrackEvts_1  strengthenEvts_1  \\\n",
       "filename                                                                \n",
       "n10k3_v1.txt                     0                0                 0   \n",
       "n10k3_v2.txt                     0                0                 0   \n",
       "n10k3_v3.txt                     0                0                 0   \n",
       "n10k3_v4.txt                     0                0                 0   \n",
       "n10k3_v5.txt                     0                0                 0   \n",
       "...                            ...              ...               ...   \n",
       "n49k4_v1.txt                     0                0                 0   \n",
       "n49k4_v2.txt                     0                0                 0   \n",
       "n49k4_v3.txt                     0                0                 0   \n",
       "n49k4_v4.txt                     0                0                 0   \n",
       "n49k4_v5.txt                     0                0                 0   \n",
       "\n",
       "              maxStackDepth_1      evts_2  ...  pruneBacktrackEvts_4  \\\n",
       "filename                                   ...                         \n",
       "n10k3_v1.txt                3          60  ...                   NaN   \n",
       "n10k3_v2.txt                3          59  ...                   NaN   \n",
       "n10k3_v3.txt                3          66  ...                   NaN   \n",
       "n10k3_v4.txt                3         106  ...                   NaN   \n",
       "n10k3_v5.txt                3          21  ...                   NaN   \n",
       "...                       ...         ...  ...                   ...   \n",
       "n49k4_v1.txt                3  2318009416  ...                   NaN   \n",
       "n49k4_v2.txt                3  2511054050  ...                   NaN   \n",
       "n49k4_v3.txt                3  2402044766  ...                   NaN   \n",
       "n49k4_v4.txt                3  2319516173  ...                   NaN   \n",
       "n49k4_v5.txt                3  2299941979  ...                   NaN   \n",
       "\n",
       "              backtrackEvts_4  strengthenEvts_4  maxStackDepth_4  evts_5  \\\n",
       "filename                                                                   \n",
       "n10k3_v1.txt              NaN               NaN              NaN     NaN   \n",
       "n10k3_v2.txt              NaN               NaN              NaN     NaN   \n",
       "n10k3_v3.txt              NaN               NaN              NaN     NaN   \n",
       "n10k3_v4.txt              NaN               NaN              NaN     NaN   \n",
       "n10k3_v5.txt              NaN               NaN              NaN     NaN   \n",
       "...                       ...               ...              ...     ...   \n",
       "n49k4_v1.txt              NaN               NaN              NaN     NaN   \n",
       "n49k4_v2.txt              NaN               NaN              NaN     NaN   \n",
       "n49k4_v3.txt              NaN               NaN              NaN     NaN   \n",
       "n49k4_v4.txt              NaN               NaN              NaN     NaN   \n",
       "n49k4_v5.txt              NaN               NaN              NaN     NaN   \n",
       "\n",
       "              expandEvts_5  pruneBacktrackEvts_5  backtrackEvts_5  \\\n",
       "filename                                                            \n",
       "n10k3_v1.txt           NaN                   NaN              NaN   \n",
       "n10k3_v2.txt           NaN                   NaN              NaN   \n",
       "n10k3_v3.txt           NaN                   NaN              NaN   \n",
       "n10k3_v4.txt           NaN                   NaN              NaN   \n",
       "n10k3_v5.txt           NaN                   NaN              NaN   \n",
       "...                    ...                   ...              ...   \n",
       "n49k4_v1.txt           NaN                   NaN              NaN   \n",
       "n49k4_v2.txt           NaN                   NaN              NaN   \n",
       "n49k4_v3.txt           NaN                   NaN              NaN   \n",
       "n49k4_v4.txt           NaN                   NaN              NaN   \n",
       "n49k4_v5.txt           NaN                   NaN              NaN   \n",
       "\n",
       "              strengthenEvts_5  maxStackDepth_5  \n",
       "filename                                         \n",
       "n10k3_v1.txt               NaN              NaN  \n",
       "n10k3_v2.txt               NaN              NaN  \n",
       "n10k3_v3.txt               NaN              NaN  \n",
       "n10k3_v4.txt               NaN              NaN  \n",
       "n10k3_v5.txt               NaN              NaN  \n",
       "...                        ...              ...  \n",
       "n49k4_v1.txt               NaN              NaN  \n",
       "n49k4_v2.txt               NaN              NaN  \n",
       "n49k4_v3.txt               NaN              NaN  \n",
       "n49k4_v4.txt               NaN              NaN  \n",
       "n49k4_v5.txt               NaN              NaN  \n",
       "\n",
       "[620 rows x 42 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a375aa-262a-49c2-a4b3-8e57d0a0b8bd",
   "metadata": {},
   "source": [
    "## 2. Create a Dataframe with input Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1da3d811-5a73-49cd-8ae3-735cc02397d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 450 instance files\n",
      "Missing 170 files\n",
      "First few missing files: ['n10k3_v1.txt', 'n10k3_v2.txt', 'n10k3_v3.txt', 'n10k3_v4.txt', 'n10k3_v5.txt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>k</th>\n",
       "      <th>total_sum</th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>max_num</th>\n",
       "      <th>min_num</th>\n",
       "      <th>avg_subset_sum</th>\n",
       "      <th>perfect_partition_sum</th>\n",
       "      <th>max_to_avg_ratio</th>\n",
       "      <th>range_to_avg_ratio</th>\n",
       "      <th>coef_of_variation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n10k3_v1.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10k3_v2.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10k3_v3.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10k3_v4.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10k3_v5.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v1.txt</th>\n",
       "      <td>49.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2084.0</td>\n",
       "      <td>784.085798</td>\n",
       "      <td>0.528426</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>521.00</td>\n",
       "      <td>521.00</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.184261</td>\n",
       "      <td>0.658385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v2.txt</th>\n",
       "      <td>49.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1848.0</td>\n",
       "      <td>729.959184</td>\n",
       "      <td>0.279890</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>462.00</td>\n",
       "      <td>462.00</td>\n",
       "      <td>0.199134</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.716380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v3.txt</th>\n",
       "      <td>49.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2055.0</td>\n",
       "      <td>724.016660</td>\n",
       "      <td>0.322909</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>513.75</td>\n",
       "      <td>513.75</td>\n",
       "      <td>0.188808</td>\n",
       "      <td>0.186861</td>\n",
       "      <td>0.641591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v4.txt</th>\n",
       "      <td>49.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2504.0</td>\n",
       "      <td>720.295710</td>\n",
       "      <td>-0.022441</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>626.00</td>\n",
       "      <td>626.00</td>\n",
       "      <td>0.153355</td>\n",
       "      <td>0.150160</td>\n",
       "      <td>0.525191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v5.txt</th>\n",
       "      <td>49.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2690.0</td>\n",
       "      <td>608.581424</td>\n",
       "      <td>-0.303635</td>\n",
       "      <td>99.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>672.50</td>\n",
       "      <td>672.50</td>\n",
       "      <td>0.147212</td>\n",
       "      <td>0.142751</td>\n",
       "      <td>0.449369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 n    k  total_sum    variance  skewness  max_num  min_num  \\\n",
       "filename                                                                     \n",
       "n10k3_v1.txt   NaN  NaN        NaN         NaN       NaN      NaN      NaN   \n",
       "n10k3_v2.txt   NaN  NaN        NaN         NaN       NaN      NaN      NaN   \n",
       "n10k3_v3.txt   NaN  NaN        NaN         NaN       NaN      NaN      NaN   \n",
       "n10k3_v4.txt   NaN  NaN        NaN         NaN       NaN      NaN      NaN   \n",
       "n10k3_v5.txt   NaN  NaN        NaN         NaN       NaN      NaN      NaN   \n",
       "...            ...  ...        ...         ...       ...      ...      ...   \n",
       "n49k4_v1.txt  49.0  4.0     2084.0  784.085798  0.528426     98.0      2.0   \n",
       "n49k4_v2.txt  49.0  4.0     1848.0  729.959184  0.279890     92.0      1.0   \n",
       "n49k4_v3.txt  49.0  4.0     2055.0  724.016660  0.322909     97.0      1.0   \n",
       "n49k4_v4.txt  49.0  4.0     2504.0  720.295710 -0.022441     96.0      2.0   \n",
       "n49k4_v5.txt  49.0  4.0     2690.0  608.581424 -0.303635     99.0      3.0   \n",
       "\n",
       "              avg_subset_sum  perfect_partition_sum  max_to_avg_ratio  \\\n",
       "filename                                                                \n",
       "n10k3_v1.txt             NaN                    NaN               NaN   \n",
       "n10k3_v2.txt             NaN                    NaN               NaN   \n",
       "n10k3_v3.txt             NaN                    NaN               NaN   \n",
       "n10k3_v4.txt             NaN                    NaN               NaN   \n",
       "n10k3_v5.txt             NaN                    NaN               NaN   \n",
       "...                      ...                    ...               ...   \n",
       "n49k4_v1.txt          521.00                 521.00          0.188100   \n",
       "n49k4_v2.txt          462.00                 462.00          0.199134   \n",
       "n49k4_v3.txt          513.75                 513.75          0.188808   \n",
       "n49k4_v4.txt          626.00                 626.00          0.153355   \n",
       "n49k4_v5.txt          672.50                 672.50          0.147212   \n",
       "\n",
       "              range_to_avg_ratio  coef_of_variation  \n",
       "filename                                             \n",
       "n10k3_v1.txt                 NaN                NaN  \n",
       "n10k3_v2.txt                 NaN                NaN  \n",
       "n10k3_v3.txt                 NaN                NaN  \n",
       "n10k3_v4.txt                 NaN                NaN  \n",
       "n10k3_v5.txt                 NaN                NaN  \n",
       "...                          ...                ...  \n",
       "n49k4_v1.txt            0.184261           0.658385  \n",
       "n49k4_v2.txt            0.196970           0.716380  \n",
       "n49k4_v3.txt            0.186861           0.641591  \n",
       "n49k4_v4.txt            0.150160           0.525191  \n",
       "n49k4_v5.txt            0.142751           0.449369  \n",
       "\n",
       "[620 rows x 12 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def process_path_features(df, instance_dir):\n",
    "    \"\"\"\n",
    "    Process input features from the original instance files.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with existing features from logs\n",
    "        instance_dir: Directory containing the original instance files\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with input features\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame for input features with filename as index\n",
    "    input_features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Track missing files\n",
    "    missing_files = []\n",
    "    processed_files = 0\n",
    "    \n",
    "    # Process each file\n",
    "    for filename in df.index:\n",
    "        file_path = os.path.join(instance_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                missing_files.append(filename)\n",
    "                continue\n",
    "                \n",
    "            # Read the instance file\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            # Extract data from file\n",
    "            solution = int(lines[0].strip())  # -1 if no solution\n",
    "            k = int(lines[1].strip())         # number of partitions\n",
    "            numbers = [int(line.strip()) for line in lines[2:]]\n",
    "            n = len(numbers)                  # number of elements\n",
    "            \n",
    "            # Calculate basic input features\n",
    "            total_sum = sum(numbers)\n",
    "            variance = np.var(numbers) if n > 1 else 0\n",
    "            skewness = stats.skew(numbers) if n > 2 else 0\n",
    "            max_num = max(numbers) if numbers else 0\n",
    "            min_num = min(numbers) if numbers else 0\n",
    "            avg_subset_sum = total_sum / k if k > 0 else 0\n",
    "            \n",
    "            # Store features\n",
    "            input_features.loc[filename, 'n'] = n\n",
    "            input_features.loc[filename, 'k'] = k\n",
    "            input_features.loc[filename, 'total_sum'] = total_sum\n",
    "            input_features.loc[filename, 'variance'] = variance\n",
    "            input_features.loc[filename, 'skewness'] = skewness\n",
    "            input_features.loc[filename, 'max_num'] = max_num\n",
    "            input_features.loc[filename, 'min_num'] = min_num\n",
    "            input_features.loc[filename, 'avg_subset_sum'] = avg_subset_sum\n",
    "            # input_features.loc[filename, 'solution_exists'] = 0 if solution == -1 else 1\n",
    "            \n",
    "            # Calculate additional features\n",
    "            # Theoretical minimum objective value (difference between max and min subset sums)\n",
    "            # In perfect partitioning, all subsets would have the same sum\n",
    "            perfect_partition = total_sum / k\n",
    "            input_features.loc[filename, 'perfect_partition_sum'] = perfect_partition\n",
    "            \n",
    "            # How close is the maximum number to the average subset sum?\n",
    "            # If max_num > avg_subset_sum, the problem is likely harder\n",
    "            input_features.loc[filename, 'max_to_avg_ratio'] = max_num / avg_subset_sum if avg_subset_sum > 0 else float('inf')\n",
    "            \n",
    "            # Range to average ratio\n",
    "            input_features.loc[filename, 'range_to_avg_ratio'] = (max_num - min_num) / avg_subset_sum if avg_subset_sum > 0 else float('inf')\n",
    "            \n",
    "            # Coefficient of variation (standardized measure of dispersion)\n",
    "            mean = np.mean(numbers)\n",
    "            std_dev = np.std(numbers)\n",
    "            input_features.loc[filename, 'coef_of_variation'] = std_dev / mean if mean > 0 else 0\n",
    "            \n",
    "            processed_files += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            missing_files.append(filename)\n",
    "    \n",
    "    print(f\"Processed {processed_files} instance files\")\n",
    "    print(f\"Missing {len(missing_files)} files\")\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"First few missing files: {missing_files[:5]}\")\n",
    "    \n",
    "    # Return a new DataFrame with input features\n",
    "    # This keeps the original df unchanged and allows for better merging later\n",
    "    return input_features\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "instance_dir = \"solver/numpart/instances/feature_collected\"\n",
    "input_features_df = process_path_features(solver_features_df, instance_dir)\n",
    "# df = extract_and_analyze_ml_features(\"ml_features.jsonl\", instance_dir)\n",
    "# df.to_csv(\"ml_features_complete.csv\", index=False)\n",
    "\n",
    "input_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d25aefbb-ba77-479c-b83e-ea4ed05175f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged features: 620 rows, 54 columns\n"
     ]
    }
   ],
   "source": [
    "def merge_features(solver_features_df, input_features_df):\n",
    "    \"\"\"\n",
    "    Merge solver features with input features.\n",
    "    \n",
    "    Args:\n",
    "        solver_features_df: DataFrame with solver features\n",
    "        input_features_df: DataFrame with input features\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Merged DataFrame\n",
    "    \"\"\"\n",
    "    # Set filename as index for solver features to enable proper joining\n",
    "    # solver_features_df = solver_features_df.set_index('filename')\n",
    "    \n",
    "    # Merge DataFrames on filename index\n",
    "    merged_df = solver_features_df.join(input_features_df, how='inner', lsuffix='_solver', rsuffix='_input')\n",
    "    \n",
    "    # Reset index to make filename a column again\n",
    "    # merged_df = merged_df.reset_index()\n",
    "    \n",
    "    print(f\"Merged features: {len(merged_df)} rows, {len(merged_df.columns)} columns\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "df = merge_features(solver_features_df, input_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27e69d27-cb74-4100-a50e-741b63de22f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_solver</th>\n",
       "      <th>k_solver</th>\n",
       "      <th>num_stackdepth3_logs</th>\n",
       "      <th>evts_1</th>\n",
       "      <th>expandEvts_1</th>\n",
       "      <th>pruneBacktrackEvts_1</th>\n",
       "      <th>backtrackEvts_1</th>\n",
       "      <th>strengthenEvts_1</th>\n",
       "      <th>maxStackDepth_1</th>\n",
       "      <th>evts_2</th>\n",
       "      <th>...</th>\n",
       "      <th>total_sum</th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>max_num</th>\n",
       "      <th>min_num</th>\n",
       "      <th>avg_subset_sum</th>\n",
       "      <th>perfect_partition_sum</th>\n",
       "      <th>max_to_avg_ratio</th>\n",
       "      <th>range_to_avg_ratio</th>\n",
       "      <th>coef_of_variation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n10k3_v1.txt</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10k3_v2.txt</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10k3_v3.txt</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10k3_v4.txt</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>106</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10k3_v5.txt</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v1.txt</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2318009416</td>\n",
       "      <td>...</td>\n",
       "      <td>2084.0</td>\n",
       "      <td>784.085798</td>\n",
       "      <td>0.528426</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>521.00</td>\n",
       "      <td>521.00</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.184261</td>\n",
       "      <td>0.658385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v2.txt</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2511054050</td>\n",
       "      <td>...</td>\n",
       "      <td>1848.0</td>\n",
       "      <td>729.959184</td>\n",
       "      <td>0.279890</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>462.00</td>\n",
       "      <td>462.00</td>\n",
       "      <td>0.199134</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.716380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v3.txt</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2402044766</td>\n",
       "      <td>...</td>\n",
       "      <td>2055.0</td>\n",
       "      <td>724.016660</td>\n",
       "      <td>0.322909</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>513.75</td>\n",
       "      <td>513.75</td>\n",
       "      <td>0.188808</td>\n",
       "      <td>0.186861</td>\n",
       "      <td>0.641591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v4.txt</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2319516173</td>\n",
       "      <td>...</td>\n",
       "      <td>2504.0</td>\n",
       "      <td>720.295710</td>\n",
       "      <td>-0.022441</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>626.00</td>\n",
       "      <td>626.00</td>\n",
       "      <td>0.153355</td>\n",
       "      <td>0.150160</td>\n",
       "      <td>0.525191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n49k4_v5.txt</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2299941979</td>\n",
       "      <td>...</td>\n",
       "      <td>2690.0</td>\n",
       "      <td>608.581424</td>\n",
       "      <td>-0.303635</td>\n",
       "      <td>99.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>672.50</td>\n",
       "      <td>672.50</td>\n",
       "      <td>0.147212</td>\n",
       "      <td>0.142751</td>\n",
       "      <td>0.449369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              n_solver  k_solver  num_stackdepth3_logs  evts_1  expandEvts_1  \\\n",
       "filename                                                                       \n",
       "n10k3_v1.txt        10         3                     3       4             4   \n",
       "n10k3_v2.txt        10         3                     3       4             4   \n",
       "n10k3_v3.txt        10         3                     3       4             4   \n",
       "n10k3_v4.txt        10         3                     3       4             4   \n",
       "n10k3_v5.txt        10         3                     3       4             4   \n",
       "...                ...       ...                   ...     ...           ...   \n",
       "n49k4_v1.txt        49         4                     1       4             4   \n",
       "n49k4_v2.txt        49         4                     1       4             4   \n",
       "n49k4_v3.txt        49         4                     1       4             4   \n",
       "n49k4_v4.txt        49         4                     1       4             4   \n",
       "n49k4_v5.txt        49         4                     1       4             4   \n",
       "\n",
       "              pruneBacktrackEvts_1  backtrackEvts_1  strengthenEvts_1  \\\n",
       "filename                                                                \n",
       "n10k3_v1.txt                     0                0                 0   \n",
       "n10k3_v2.txt                     0                0                 0   \n",
       "n10k3_v3.txt                     0                0                 0   \n",
       "n10k3_v4.txt                     0                0                 0   \n",
       "n10k3_v5.txt                     0                0                 0   \n",
       "...                            ...              ...               ...   \n",
       "n49k4_v1.txt                     0                0                 0   \n",
       "n49k4_v2.txt                     0                0                 0   \n",
       "n49k4_v3.txt                     0                0                 0   \n",
       "n49k4_v4.txt                     0                0                 0   \n",
       "n49k4_v5.txt                     0                0                 0   \n",
       "\n",
       "              maxStackDepth_1      evts_2  ...  total_sum    variance  \\\n",
       "filename                                   ...                          \n",
       "n10k3_v1.txt                3          60  ...        NaN         NaN   \n",
       "n10k3_v2.txt                3          59  ...        NaN         NaN   \n",
       "n10k3_v3.txt                3          66  ...        NaN         NaN   \n",
       "n10k3_v4.txt                3         106  ...        NaN         NaN   \n",
       "n10k3_v5.txt                3          21  ...        NaN         NaN   \n",
       "...                       ...         ...  ...        ...         ...   \n",
       "n49k4_v1.txt                3  2318009416  ...     2084.0  784.085798   \n",
       "n49k4_v2.txt                3  2511054050  ...     1848.0  729.959184   \n",
       "n49k4_v3.txt                3  2402044766  ...     2055.0  724.016660   \n",
       "n49k4_v4.txt                3  2319516173  ...     2504.0  720.295710   \n",
       "n49k4_v5.txt                3  2299941979  ...     2690.0  608.581424   \n",
       "\n",
       "              skewness  max_num  min_num  avg_subset_sum  \\\n",
       "filename                                                   \n",
       "n10k3_v1.txt       NaN      NaN      NaN             NaN   \n",
       "n10k3_v2.txt       NaN      NaN      NaN             NaN   \n",
       "n10k3_v3.txt       NaN      NaN      NaN             NaN   \n",
       "n10k3_v4.txt       NaN      NaN      NaN             NaN   \n",
       "n10k3_v5.txt       NaN      NaN      NaN             NaN   \n",
       "...                ...      ...      ...             ...   \n",
       "n49k4_v1.txt  0.528426     98.0      2.0          521.00   \n",
       "n49k4_v2.txt  0.279890     92.0      1.0          462.00   \n",
       "n49k4_v3.txt  0.322909     97.0      1.0          513.75   \n",
       "n49k4_v4.txt -0.022441     96.0      2.0          626.00   \n",
       "n49k4_v5.txt -0.303635     99.0      3.0          672.50   \n",
       "\n",
       "              perfect_partition_sum  max_to_avg_ratio  range_to_avg_ratio  \\\n",
       "filename                                                                    \n",
       "n10k3_v1.txt                    NaN               NaN                 NaN   \n",
       "n10k3_v2.txt                    NaN               NaN                 NaN   \n",
       "n10k3_v3.txt                    NaN               NaN                 NaN   \n",
       "n10k3_v4.txt                    NaN               NaN                 NaN   \n",
       "n10k3_v5.txt                    NaN               NaN                 NaN   \n",
       "...                             ...               ...                 ...   \n",
       "n49k4_v1.txt                 521.00          0.188100            0.184261   \n",
       "n49k4_v2.txt                 462.00          0.199134            0.196970   \n",
       "n49k4_v3.txt                 513.75          0.188808            0.186861   \n",
       "n49k4_v4.txt                 626.00          0.153355            0.150160   \n",
       "n49k4_v5.txt                 672.50          0.147212            0.142751   \n",
       "\n",
       "              coef_of_variation  \n",
       "filename                         \n",
       "n10k3_v1.txt                NaN  \n",
       "n10k3_v2.txt                NaN  \n",
       "n10k3_v3.txt                NaN  \n",
       "n10k3_v4.txt                NaN  \n",
       "n10k3_v5.txt                NaN  \n",
       "...                         ...  \n",
       "n49k4_v1.txt           0.658385  \n",
       "n49k4_v2.txt           0.716380  \n",
       "n49k4_v3.txt           0.641591  \n",
       "n49k4_v4.txt           0.525191  \n",
       "n49k4_v5.txt           0.449369  \n",
       "\n",
       "[620 rows x 54 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf320e-7b76-4f55-9f73-5f82595d7eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features(df):\n",
    "    \"\"\"\n",
    "    Perform basic analysis on the extracted features.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with extracted features\n",
    "        \n",
    "    Returns:\n",
    "        dict: Basic statistics about the features\n",
    "    \"\"\"\n",
    "    stats_dict = {\n",
    "        'total_instances': len(df),\n",
    "        'timeout_instances': df['censored'].sum() if 'censored' in df.columns else 'N/A',\n",
    "        'complete_instances': (len(df) - df['censored'].sum()) if 'censored' in df.columns else 'N/A',\n",
    "    }\n",
    "    \n",
    "    # Add statistics for key numeric columns\n",
    "    numeric_cols = ['final_expandEvts', 'final_maxStackDepth', 'total_sum', 'variance', \n",
    "                    'max_num', 'avg_subset_sum'] \n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            stats_dict[f'avg_{col}'] = df[col].mean()\n",
    "            stats_dict[f'max_{col}'] = df[col].max()\n",
    "    \n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    for key, value in stats_dict.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    return stats_dict\n",
    "\n",
    "def extract_and_analyze_ml_features(jsonl_path, instance_dir):\n",
    "    \"\"\"\n",
    "    Main function to extract and analyze ML features.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_path: Path to the ml_features.jsonl file\n",
    "        instance_dir: Directory containing original instance files\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with extracted features\n",
    "    \"\"\"\n",
    "    print(f\"Processing {jsonl_path}...\")\n",
    "    \n",
    "    # Extract solver features\n",
    "    solver_features_df = extract_ml_features(jsonl_path)\n",
    "    \n",
    "    # Extract input features\n",
    "    print(f\"\\nProcessing instance files from {instance_dir}...\")\n",
    "    input_features_df = process_path_features(solver_features_df, instance_dir)\n",
    "    \n",
    "    # Merge features\n",
    "    merged_df = merge_features(solver_features_df, input_features_df)\n",
    "    \n",
    "    # Analyze features\n",
    "    analyze_features(merged_df)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de59553-1893-498e-9b26-9891b5cee84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_and_analyze_ml_features(jsonl_path, instance_dir=None):\n",
    "    \"\"\"\n",
    "    Main function to extract and analyze ML features.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_path: Path to the ml_features.jsonl file\n",
    "        instance_dir: Optional directory containing original instance files\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with extracted features\n",
    "    \"\"\"\n",
    "    print(f\"Processing {jsonl_path}...\")\n",
    "    \n",
    "    # Extract basic features\n",
    "    df = extract_ml_features(jsonl_path)\n",
    "    \n",
    "    # Process path-based features if instance directory is provided\n",
    "    if instance_dir:\n",
    "        df = process_path_features(df, instance_dir)\n",
    "    \n",
    "    # Analyze features\n",
    "    analyze_features(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b10853d-b10e-494b-ae6b-4f239bdd8347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'FilePath | ReadBuffer[str] | ReadBuffer[bytes]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0morient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtyp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Literal['frame', 'series']\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'frame'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DtypeArg | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconvert_axes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconvert_dates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | list[str]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeep_default_dates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprecise_float\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdate_unit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding_errors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'strict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchunksize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'CompressionOptions'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnrows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'StorageOptions | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DtypeBackend | lib.NoDefault'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'JSONEngine'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ujson'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'DataFrame | Series | JsonReader'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Convert a JSON string to pandas object.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "path_or_buf : a valid JSON str, path object or file-like object\n",
       "    Any valid string path is acceptable. The string could be a URL. Valid\n",
       "    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n",
       "    expected. A local file could be:\n",
       "    ``file://localhost/path/to/table.json``.\n",
       "\n",
       "    If you want to pass in a path object, pandas accepts any\n",
       "    ``os.PathLike``.\n",
       "\n",
       "    By file-like object, we refer to objects with a ``read()`` method,\n",
       "    such as a file handle (e.g. via builtin ``open`` function)\n",
       "    or ``StringIO``.\n",
       "\n",
       "    .. deprecated:: 2.1.0\n",
       "        Passing json literal strings is deprecated.\n",
       "\n",
       "orient : str, optional\n",
       "    Indication of expected JSON string format.\n",
       "    Compatible JSON strings can be produced by ``to_json()`` with a\n",
       "    corresponding orient value.\n",
       "    The set of possible orients is:\n",
       "\n",
       "    - ``'split'`` : dict like\n",
       "      ``{index -> [index], columns -> [columns], data -> [values]}``\n",
       "    - ``'records'`` : list like\n",
       "      ``[{column -> value}, ... , {column -> value}]``\n",
       "    - ``'index'`` : dict like ``{index -> {column -> value}}``\n",
       "    - ``'columns'`` : dict like ``{column -> {index -> value}}``\n",
       "    - ``'values'`` : just the values array\n",
       "    - ``'table'`` : dict like ``{'schema': {schema}, 'data': {data}}``\n",
       "\n",
       "    The allowed and default values depend on the value\n",
       "    of the `typ` parameter.\n",
       "\n",
       "    * when ``typ == 'series'``,\n",
       "\n",
       "      - allowed orients are ``{'split','records','index'}``\n",
       "      - default is ``'index'``\n",
       "      - The Series index must be unique for orient ``'index'``.\n",
       "\n",
       "    * when ``typ == 'frame'``,\n",
       "\n",
       "      - allowed orients are ``{'split','records','index',\n",
       "        'columns','values', 'table'}``\n",
       "      - default is ``'columns'``\n",
       "      - The DataFrame index must be unique for orients ``'index'`` and\n",
       "        ``'columns'``.\n",
       "      - The DataFrame columns must be unique for orients ``'index'``,\n",
       "        ``'columns'``, and ``'records'``.\n",
       "\n",
       "typ : {'frame', 'series'}, default 'frame'\n",
       "    The type of object to recover.\n",
       "\n",
       "dtype : bool or dict, default None\n",
       "    If True, infer dtypes; if a dict of column to dtype, then use those;\n",
       "    if False, then don't infer dtypes at all, applies only to the data.\n",
       "\n",
       "    For all ``orient`` values except ``'table'``, default is True.\n",
       "\n",
       "convert_axes : bool, default None\n",
       "    Try to convert the axes to the proper dtypes.\n",
       "\n",
       "    For all ``orient`` values except ``'table'``, default is True.\n",
       "\n",
       "convert_dates : bool or list of str, default True\n",
       "    If True then default datelike columns may be converted (depending on\n",
       "    keep_default_dates).\n",
       "    If False, no dates will be converted.\n",
       "    If a list of column names, then those columns will be converted and\n",
       "    default datelike columns may also be converted (depending on\n",
       "    keep_default_dates).\n",
       "\n",
       "keep_default_dates : bool, default True\n",
       "    If parsing dates (convert_dates is not False), then try to parse the\n",
       "    default datelike columns.\n",
       "    A column label is datelike if\n",
       "\n",
       "    * it ends with ``'_at'``,\n",
       "\n",
       "    * it ends with ``'_time'``,\n",
       "\n",
       "    * it begins with ``'timestamp'``,\n",
       "\n",
       "    * it is ``'modified'``, or\n",
       "\n",
       "    * it is ``'date'``.\n",
       "\n",
       "precise_float : bool, default False\n",
       "    Set to enable usage of higher precision (strtod) function when\n",
       "    decoding string to double values. Default (False) is to use fast but\n",
       "    less precise builtin functionality.\n",
       "\n",
       "date_unit : str, default None\n",
       "    The timestamp unit to detect if converting dates. The default behaviour\n",
       "    is to try and detect the correct precision, but if this is not desired\n",
       "    then pass one of 's', 'ms', 'us' or 'ns' to force parsing only seconds,\n",
       "    milliseconds, microseconds or nanoseconds respectively.\n",
       "\n",
       "encoding : str, default is 'utf-8'\n",
       "    The encoding to use to decode py3 bytes.\n",
       "\n",
       "encoding_errors : str, optional, default \"strict\"\n",
       "    How encoding errors are treated. `List of possible values\n",
       "    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
       "\n",
       "    .. versionadded:: 1.3.0\n",
       "\n",
       "lines : bool, default False\n",
       "    Read the file as a json object per line.\n",
       "\n",
       "chunksize : int, optional\n",
       "    Return JsonReader object for iteration.\n",
       "    See the `line-delimited json docs\n",
       "    <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json>`_\n",
       "    for more information on ``chunksize``.\n",
       "    This can only be passed if `lines=True`.\n",
       "    If this is None, the file will be read into memory all at once.\n",
       "compression : str or dict, default 'infer'\n",
       "    For on-the-fly decompression of on-disk data. If 'infer' and 'path_or_buf' is\n",
       "    path-like, then detect compression from the following extensions: '.gz',\n",
       "    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
       "    (otherwise no compression).\n",
       "    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
       "    Set to ``None`` for no decompression.\n",
       "    Can also be a dict with key ``'method'`` set\n",
       "    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
       "    other key-value pairs are forwarded to\n",
       "    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
       "    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n",
       "    ``tarfile.TarFile``, respectively.\n",
       "    As an example, the following could be passed for Zstandard decompression using a\n",
       "    custom compression dictionary:\n",
       "    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
       "\n",
       "    .. versionadded:: 1.5.0\n",
       "        Added support for `.tar` files.\n",
       "\n",
       "    .. versionchanged:: 1.4.0 Zstandard support.\n",
       "\n",
       "nrows : int, optional\n",
       "    The number of lines from the line-delimited jsonfile that has to be read.\n",
       "    This can only be passed if `lines=True`.\n",
       "    If this is None, all the rows will be returned.\n",
       "\n",
       "storage_options : dict, optional\n",
       "    Extra options that make sense for a particular storage connection, e.g.\n",
       "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
       "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
       "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
       "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
       "    details, and for more examples on storage options refer `here\n",
       "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
       "    highlight=storage_options#reading-writing-remote-files>`_.\n",
       "\n",
       "dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n",
       "    Back-end data type applied to the resultant :class:`DataFrame`\n",
       "    (still experimental). Behaviour is as follows:\n",
       "\n",
       "    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
       "      (default).\n",
       "    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
       "      DataFrame.\n",
       "\n",
       "    .. versionadded:: 2.0\n",
       "\n",
       "engine : {\"ujson\", \"pyarrow\"}, default \"ujson\"\n",
       "    Parser engine to use. The ``\"pyarrow\"`` engine is only available when\n",
       "    ``lines=True``.\n",
       "\n",
       "    .. versionadded:: 2.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "Series, DataFrame, or pandas.api.typing.JsonReader\n",
       "    A JsonReader is returned when ``chunksize`` is not ``0`` or ``None``.\n",
       "    Otherwise, the type returned depends on the value of ``typ``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DataFrame.to_json : Convert a DataFrame to a JSON string.\n",
       "Series.to_json : Convert a Series to a JSON string.\n",
       "json_normalize : Normalize semi-structured JSON data into a flat table.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Specific to ``orient='table'``, if a :class:`DataFrame` with a literal\n",
       ":class:`Index` name of `index` gets written with :func:`to_json`, the\n",
       "subsequent read operation will incorrectly set the :class:`Index` name to\n",
       "``None``. This is because `index` is also used by :func:`DataFrame.to_json`\n",
       "to denote a missing :class:`Index` name, and the subsequent\n",
       ":func:`read_json` operation cannot distinguish between the two. The same\n",
       "limitation is encountered with a :class:`MultiIndex` and any names\n",
       "beginning with ``'level_'``.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from io import StringIO\n",
       ">>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],\n",
       "...                   index=['row 1', 'row 2'],\n",
       "...                   columns=['col 1', 'col 2'])\n",
       "\n",
       "Encoding/decoding a Dataframe using ``'split'`` formatted JSON:\n",
       "\n",
       ">>> df.to_json(orient='split')\n",
       "    '{\"columns\":[\"col 1\",\"col 2\"],\"index\":[\"row 1\",\"row 2\"],\"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]}'\n",
       ">>> pd.read_json(StringIO(_), orient='split')\n",
       "      col 1 col 2\n",
       "row 1     a     b\n",
       "row 2     c     d\n",
       "\n",
       "Encoding/decoding a Dataframe using ``'index'`` formatted JSON:\n",
       "\n",
       ">>> df.to_json(orient='index')\n",
       "'{\"row 1\":{\"col 1\":\"a\",\"col 2\":\"b\"},\"row 2\":{\"col 1\":\"c\",\"col 2\":\"d\"}}'\n",
       "\n",
       ">>> pd.read_json(StringIO(_), orient='index')\n",
       "      col 1 col 2\n",
       "row 1     a     b\n",
       "row 2     c     d\n",
       "\n",
       "Encoding/decoding a Dataframe using ``'records'`` formatted JSON.\n",
       "Note that index labels are not preserved with this encoding.\n",
       "\n",
       ">>> df.to_json(orient='records')\n",
       "'[{\"col 1\":\"a\",\"col 2\":\"b\"},{\"col 1\":\"c\",\"col 2\":\"d\"}]'\n",
       ">>> pd.read_json(StringIO(_), orient='records')\n",
       "  col 1 col 2\n",
       "0     a     b\n",
       "1     c     d\n",
       "\n",
       "Encoding with Table Schema\n",
       "\n",
       ">>> df.to_json(orient='table')\n",
       "    '{\"schema\":{\"fields\":[{\"name\":\"index\",\"type\":\"string\"},{\"name\":\"col 1\",\"type\":\"string\"},{\"name\":\"col 2\",\"type\":\"string\"}],\"primaryKey\":[\"index\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"index\":\"row 1\",\"col 1\":\"a\",\"col 2\":\"b\"},{\"index\":\"row 2\",\"col 1\":\"c\",\"col 2\":\"d\"}]}'\n",
       "\n",
       "The following example uses ``dtype_backend=\"numpy_nullable\"``\n",
       "\n",
       ">>> data = '''{\"index\": {\"0\": 0, \"1\": 1},\n",
       "...        \"a\": {\"0\": 1, \"1\": null},\n",
       "...        \"b\": {\"0\": 2.5, \"1\": 4.5},\n",
       "...        \"c\": {\"0\": true, \"1\": false},\n",
       "...        \"d\": {\"0\": \"a\", \"1\": \"b\"},\n",
       "...        \"e\": {\"0\": 1577.2, \"1\": 1577.1}}'''\n",
       ">>> pd.read_json(StringIO(data), dtype_backend=\"numpy_nullable\")\n",
       "   index     a    b      c  d       e\n",
       "0      0     1  2.5   True  a  1577.2\n",
       "1      1  <NA>  4.5  False  b  1577.1\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.10/site-packages/pandas/io/json/_json.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.read_json?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65047797-f623-4cea-a69f-4b44d6433bc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['n10k3_v1.txt', 'n10k3_v2.txt', 'n10k3_v3.txt', 'n10k3_v4.txt',\n",
       "       'n10k3_v5.txt', 'n11k3_v1.txt', 'n11k3_v2.txt', 'n11k3_v3.txt',\n",
       "       'n11k3_v4.txt', 'n11k3_v5.txt',\n",
       "       ...\n",
       "       'n49k3_v1.txt', 'n49k3_v2.txt', 'n49k3_v3.txt', 'n49k3_v4.txt',\n",
       "       'n49k3_v5.txt', 'n49k4_v1.txt', 'n49k4_v2.txt', 'n49k4_v3.txt',\n",
       "       'n49k4_v4.txt', 'n49k4_v5.txt'],\n",
       "      dtype='object', name='filename', length=620)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features_df.index\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-manipulation-class",
   "language": "python",
   "name": "data-manipulation-class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
