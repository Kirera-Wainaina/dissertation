{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114235da-20e8-4262-87d4-b00b5c9c30fe",
   "metadata": {},
   "source": [
    "# Analyse the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944efda5-c0cc-4db9-81c3-524688cd9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b349c-7171-4c85-a441-28684b0565ff",
   "metadata": {},
   "source": [
    "## load data\n",
    "Load the data with 0 for empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df626836-158c-4823-9536-3e1ccf41f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_to_df(filename):\n",
    "    \"\"\"\n",
    "    Load an Excel file into a pandas DataFrame and replace empty values with 0.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    filename : str\n",
    "        Path to the Excel file (.xlsx)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the Excel data with empty values replaced by 0\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"The file {filename} does not exist.\")\n",
    "\n",
    "    # Check if file is an Excel file\n",
    "    if not filename.endswith('.xlsx'):\n",
    "        raise ValueError(f\"The file {filename} is not an Excel file (.xlsx).\")\n",
    "\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(filename)\n",
    "\n",
    "        # Replace empty values (NaN) with 0\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading Excel file: {str(e)}\")\n",
    "\n",
    "df = load_excel_to_df('structured_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad715a7-89a5-495d-a8b1-a17665c3fe63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_stackdepth3_logs</th>\n",
       "      <th>evts_1</th>\n",
       "      <th>expandEvts_1</th>\n",
       "      <th>pruneBacktrackEvts_1</th>\n",
       "      <th>backtrackEvts_1</th>\n",
       "      <th>strengthenEvts_1</th>\n",
       "      <th>maxStackDepth_1</th>\n",
       "      <th>evts_2</th>\n",
       "      <th>expandEvts_2</th>\n",
       "      <th>pruneBacktrackEvts_2</th>\n",
       "      <th>...</th>\n",
       "      <th>expandEvts_ratio_1</th>\n",
       "      <th>pruneBacktrackEvts_ratio_1</th>\n",
       "      <th>expandEvts_ratio_2</th>\n",
       "      <th>pruneBacktrackEvts_ratio_2</th>\n",
       "      <th>expandEvts_ratio_3</th>\n",
       "      <th>pruneBacktrackEvts_ratio_3</th>\n",
       "      <th>expandEvts_ratio_4</th>\n",
       "      <th>pruneBacktrackEvts_ratio_4</th>\n",
       "      <th>expandEvts_ratio_5</th>\n",
       "      <th>pruneBacktrackEvts_ratio_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>690.000000</td>\n",
       "      <td>690.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>6.900000e+02</td>\n",
       "      <td>6.900000e+02</td>\n",
       "      <td>6.900000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>690.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>690.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.326087</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.342984e+09</td>\n",
       "      <td>6.714918e+08</td>\n",
       "      <td>6.669157e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.506421</td>\n",
       "      <td>0.471248</td>\n",
       "      <td>0.251681</td>\n",
       "      <td>0.231418</td>\n",
       "      <td>0.134380</td>\n",
       "      <td>0.127155</td>\n",
       "      <td>0.059995</td>\n",
       "      <td>0.057876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.488042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.283705e+09</td>\n",
       "      <td>6.418524e+08</td>\n",
       "      <td>6.368330e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018576</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.251226</td>\n",
       "      <td>0.233928</td>\n",
       "      <td>0.224039</td>\n",
       "      <td>0.213020</td>\n",
       "      <td>0.163525</td>\n",
       "      <td>0.157937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.493421</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.017850e+04</td>\n",
       "      <td>5.089750e+03</td>\n",
       "      <td>4.982000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.487590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.895630e+09</td>\n",
       "      <td>9.478150e+08</td>\n",
       "      <td>9.475620e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.498040</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.248718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.617559e+09</td>\n",
       "      <td>1.308780e+09</td>\n",
       "      <td>1.288496e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.499755</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.490346</td>\n",
       "      <td>0.499356</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.013720e+09</td>\n",
       "      <td>1.506860e+09</td>\n",
       "      <td>1.480088e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.499999</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.499999</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.499999</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.499999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_stackdepth3_logs  evts_1  expandEvts_1  pruneBacktrackEvts_1  \\\n",
       "count            690.000000   690.0         690.0                 690.0   \n",
       "mean               2.326087     4.0           4.0                   0.0   \n",
       "std                1.488042     0.0           0.0                   0.0   \n",
       "min                1.000000     4.0           4.0                   0.0   \n",
       "25%                1.000000     4.0           4.0                   0.0   \n",
       "50%                2.000000     4.0           4.0                   0.0   \n",
       "75%                4.000000     4.0           4.0                   0.0   \n",
       "max                5.000000     4.0           4.0                   0.0   \n",
       "\n",
       "       backtrackEvts_1  strengthenEvts_1  maxStackDepth_1        evts_2  \\\n",
       "count            690.0             690.0            690.0  6.900000e+02   \n",
       "mean               0.0               0.0              3.0  1.342984e+09   \n",
       "std                0.0               0.0              0.0  1.283705e+09   \n",
       "min                0.0               0.0              3.0  1.100000e+01   \n",
       "25%                0.0               0.0              3.0  1.017850e+04   \n",
       "50%                0.0               0.0              3.0  1.895630e+09   \n",
       "75%                0.0               0.0              3.0  2.617559e+09   \n",
       "max                0.0               0.0              3.0  3.013720e+09   \n",
       "\n",
       "       expandEvts_2  pruneBacktrackEvts_2  ...  expandEvts_ratio_1  \\\n",
       "count  6.900000e+02          6.900000e+02  ...               690.0   \n",
       "mean   6.714918e+08          6.669157e+08  ...                 1.0   \n",
       "std    6.418524e+08          6.368330e+08  ...                 0.0   \n",
       "min    6.000000e+00          2.000000e+00  ...                 1.0   \n",
       "25%    5.089750e+03          4.982000e+03  ...                 1.0   \n",
       "50%    9.478150e+08          9.475620e+08  ...                 1.0   \n",
       "75%    1.308780e+09          1.288496e+09  ...                 1.0   \n",
       "max    1.506860e+09          1.480088e+09  ...                 1.0   \n",
       "\n",
       "       pruneBacktrackEvts_ratio_1  expandEvts_ratio_2  \\\n",
       "count                       690.0          690.000000   \n",
       "mean                          0.0            0.506421   \n",
       "std                           0.0            0.018576   \n",
       "min                           0.0            0.493421   \n",
       "25%                           0.0            0.500000   \n",
       "50%                           0.0            0.500000   \n",
       "75%                           0.0            0.500000   \n",
       "max                           0.0            0.636364   \n",
       "\n",
       "       pruneBacktrackEvts_ratio_2  expandEvts_ratio_3  \\\n",
       "count                  690.000000          690.000000   \n",
       "mean                     0.471248            0.251681   \n",
       "std                      0.060241            0.251226   \n",
       "min                      0.181818            0.000000   \n",
       "25%                      0.487590            0.000000   \n",
       "50%                      0.498040            0.428571   \n",
       "75%                      0.499755            0.500000   \n",
       "max                      0.499999            0.615385   \n",
       "\n",
       "       pruneBacktrackEvts_ratio_3  expandEvts_ratio_4  \\\n",
       "count                  690.000000          690.000000   \n",
       "mean                     0.231418            0.134380   \n",
       "std                      0.233928            0.224039   \n",
       "min                      0.000000            0.000000   \n",
       "25%                      0.000000            0.000000   \n",
       "50%                      0.248718            0.000000   \n",
       "75%                      0.490346            0.499356   \n",
       "max                      0.499999            0.588235   \n",
       "\n",
       "       pruneBacktrackEvts_ratio_4  expandEvts_ratio_5  \\\n",
       "count                  690.000000          690.000000   \n",
       "mean                     0.127155            0.059995   \n",
       "std                      0.213020            0.163525   \n",
       "min                      0.000000            0.000000   \n",
       "25%                      0.000000            0.000000   \n",
       "50%                      0.000000            0.000000   \n",
       "75%                      0.360000            0.000000   \n",
       "max                      0.499999            0.534884   \n",
       "\n",
       "       pruneBacktrackEvts_ratio_5  \n",
       "count                  690.000000  \n",
       "mean                     0.057876  \n",
       "std                      0.157937  \n",
       "min                      0.000000  \n",
       "25%                      0.000000  \n",
       "50%                      0.000000  \n",
       "75%                      0.000000  \n",
       "max                      0.499999  \n",
       "\n",
       "[8 rows x 62 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "364f2692-7030-4f34-b942-951803452ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding log-scaled features...\n",
      "Imputing missing log features...\n",
      "Selected features: ['num_stackdepth3_logs', 'evts_1', 'expandEvts_1', 'pruneBacktrackEvts_1', 'backtrackEvts_1', 'strengthenEvts_1', 'maxStackDepth_1', 'evts_2', 'expandEvts_2', 'pruneBacktrackEvts_2', 'backtrackEvts_2', 'strengthenEvts_2', 'maxStackDepth_2', 'evts_3', 'expandEvts_3', 'pruneBacktrackEvts_3', 'backtrackEvts_3', 'strengthenEvts_3', 'maxStackDepth_3', 'censored', 'avg_evts', 'max_evts', 'avg_expandEvts', 'max_expandEvts', 'avg_pruneBacktrackEvts', 'max_pruneBacktrackEvts', 'evts_4', 'expandEvts_4', 'pruneBacktrackEvts_4', 'backtrackEvts_4', 'strengthenEvts_4', 'maxStackDepth_4', 'evts_5', 'expandEvts_5', 'pruneBacktrackEvts_5', 'backtrackEvts_5', 'strengthenEvts_5', 'maxStackDepth_5', 'n', 'k', 'total_sum', 'variance', 'skewness', 'max_num', 'min_num', 'avg_subset_sum', 'max_to_avg_ratio', 'range_to_avg_ratio', 'coef_of_variation', 'expandEvts_ratio_1', 'pruneBacktrackEvts_ratio_1', 'expandEvts_ratio_2', 'pruneBacktrackEvts_ratio_2', 'expandEvts_ratio_3', 'pruneBacktrackEvts_ratio_3', 'expandEvts_ratio_4', 'pruneBacktrackEvts_ratio_4', 'expandEvts_ratio_5', 'pruneBacktrackEvts_ratio_5', 'log_evts_1', 'log_expandEvts_1', 'log_pruneBacktrackEvts_1', 'log_backtrackEvts_1', 'log_strengthenEvts_1', 'log_evts_2', 'log_expandEvts_2', 'log_pruneBacktrackEvts_2', 'log_backtrackEvts_2', 'log_strengthenEvts_2', 'log_evts_3', 'log_expandEvts_3', 'log_pruneBacktrackEvts_3', 'log_backtrackEvts_3', 'log_strengthenEvts_3', 'log_evts_4', 'log_expandEvts_4', 'log_pruneBacktrackEvts_4', 'log_backtrackEvts_4', 'log_strengthenEvts_4', 'log_evts_5', 'log_expandEvts_5', 'log_pruneBacktrackEvts_5', 'log_backtrackEvts_5', 'log_strengthenEvts_5']\n",
      "\n",
      "Target (final_expandEvts) statistics:\n",
      "Mean (censored=0): 53227800.02\n",
      "Std (censored=0): 174281183.67\n",
      "Mean (all data): 761155886.29\n",
      "Std (all data): 636531467.58\n",
      "\n",
      "Training Model 1 (censored=0)...\n",
      "Error in train_random_forest_model: name 'cross_val_score' is not defined\n"
     ]
    }
   ],
   "source": [
    "def train_random_forest_model(df, include_ratios=True, save_plots=False, output_dir='plots'):\n",
    "    \"\"\"\n",
    "    Trains two Random Forest models to predict final_expandEvts: one on censored=0 data,\n",
    "    one on all data with censored as a feature. Outputs RMSE, MAPE, and feature importance.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): Input DataFrame with solver features, ratio features, and final_expandEvts.\n",
    "    - include_ratios (bool): If True, includes ratio features; if False, excludes them.\n",
    "    - save_plots (bool): If True, saves scatter and feature importance plots to output_dir.\n",
    "    - output_dir (str): Directory to save plots (default: 'plots').\n",
    "\n",
    "    Returns:\n",
    "    - dict: Contains RMSE, MAPE, feature importance DataFrames, and test predictions for both models.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a copy to avoid modifying the input DataFrame\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Ensure missing log features are imputed\n",
    "        print(\"Imputing missing log features...\")\n",
    "        for i in range(1, 6):\n",
    "            for feature in ['evts', 'expandEvts', 'pruneBacktrackEvts', 'backtrackEvts', 'strengthenEvts', 'maxStackDepth']:\n",
    "                col = f'{feature}_{i}'\n",
    "                if col in df:\n",
    "                    df.loc[df['k'] == 3, col] = df.loc[df['k'] == 3, col].fillna(0)\n",
    "                    df.loc[df['num_stackdepth3_logs'] < i, col] = df.loc[df['num_stackdepth3_logs'] < i, col].fillna(0)\n",
    "        \n",
    "        # Feature selection\n",
    "        exclude_cols = ['filename', 'final_expandEvts', 'stop_iter', 'final_maxStackDepth']\n",
    "        if not include_ratios:\n",
    "            exclude_cols.extend([f'expandEvts_ratio_{i}' for i in range(1, 6)])\n",
    "            exclude_cols.extend([f'pruneBacktrackEvts_ratio_{i}' for i in range(1, 6)])\n",
    "        features = [col for col in df.columns if col not in exclude_cols]\n",
    "        print(\"Selected features:\", features)\n",
    "        \n",
    "        # Print target statistics for context\n",
    "        print(\"\\nTarget (final_expandEvts) statistics:\")\n",
    "        print(f\"Mean (censored=0): {df[df['censored'] == 0]['final_expandEvts'].mean():.2f}\")\n",
    "        print(f\"Std (censored=0): {df[df['censored'] == 0]['final_expandEvts'].std():.2f}\")\n",
    "        print(f\"Mean (all data): {df['final_expandEvts'].mean():.2f}\")\n",
    "        print(f\"Std (all data): {df['final_expandEvts'].std():.2f}\")\n",
    "        \n",
    "        # Calculate MAPE\n",
    "        def calculate_mape(y_true, y_pred):\n",
    "            mask = y_true != 0  # Avoid division by zero\n",
    "            return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if mask.sum() > 0 else np.nan\n",
    "        \n",
    "        # Model 1: Censored=0\n",
    "        print(\"\\nTraining Model 1 (censored=0)...\")\n",
    "        df_censored = df[df['censored'] == 0]\n",
    "        if df_censored.empty:\n",
    "            print(\"Warning: No censored=0 instances found. Skipping Model 1.\")\n",
    "            rmse_censored = None\n",
    "            mape_censored = None\n",
    "            importance_censored = None\n",
    "            y_test_c = None\n",
    "            y_pred_c = None\n",
    "        else:\n",
    "            X_censored = df_censored[features]\n",
    "            y_censored = df_censored['final_expandEvts']\n",
    "            X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_censored, y_censored, test_size=0.2, random_state=42)\n",
    "            \n",
    "            rf_censored = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            rf_censored.fit(X_train_c, y_train_c)\n",
    "            y_pred_c = rf_censored.predict(X_test_c)\n",
    "            rmse_censored = np.sqrt(mean_squared_error(y_test_c, y_pred_c))\n",
    "            mape_censored = calculate_mape(y_test_c, y_pred_c)\n",
    "            print(f\"Censored=0 RMSE: {rmse_censored:.4f}\")\n",
    "            print(f\"Censored=0 MAPE: {mape_censored:.2f}%\")\n",
    "            \n",
    "            importance_censored = pd.DataFrame({\n",
    "                'feature': X_censored.columns,\n",
    "                'importance': rf_censored.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            print(\"\\nCensored=0 Feature Importance (Top 10):\")\n",
    "            print(importance_censored.head(10))\n",
    "        \n",
    "        # Model 2: All data with censored as a feature\n",
    "        print(\"\\nTraining Model 2 (all data with censored feature)...\")\n",
    "        X_all = df[features + ['censored']]\n",
    "        y_all = df['final_expandEvts']\n",
    "        X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "        \n",
    "        rf_all = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf_all.fit(X_train_a, y_train_a)\n",
    "        y_pred_a = rf_all.predict(X_test_a)\n",
    "        rmse_all = np.sqrt(mean_squared_error(y_test_a, y_pred_a))\n",
    "        mape_all = calculate_mape(y_test_a, y_pred_a)\n",
    "        print(f\"All Data RMSE: {rmse_all:.4f}\")\n",
    "        print(f\"All Data MAPE: {mape_all:.2f}%\")\n",
    "        \n",
    "        importance_all = pd.DataFrame({\n",
    "            'feature': X_all.columns,\n",
    "            'importance': rf_all.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(\"\\nAll Data Feature Importance (Top 10):\")\n",
    "        print(importance_all.head(10))\n",
    "        \n",
    "        # Plotting\n",
    "        if save_plots:\n",
    "            try:\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                \n",
    "                # Scatter plot for censored=0 model\n",
    "                if not df_censored.empty:\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "                    plt.scatter(y_test_c, y_pred_c, alpha=0.7)\n",
    "                    plt.plot([y_test_c.min(), y_test_c.max()], [y_test_c.min(), y_test_c.max()], 'r--')\n",
    "                    plt.xlabel('Actual final_expandEvts')\n",
    "                    plt.ylabel('Predicted final_expandEvts')\n",
    "                    plt.title('Censored=0: Predicted vs Actual')\n",
    "                    plt.tight_layout()\n",
    "                    scatter_path = os.path.join(output_dir, 'scatter_plot_censored.png')\n",
    "                    plt.savefig(scatter_path)\n",
    "                    plt.close()\n",
    "                    print(f\"Saved scatter plot to {scatter_path}\")\n",
    "                \n",
    "                # Feature importance plot for censored=0 model\n",
    "                if not df_censored.empty:\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "                    plt.barh(importance_censored['feature'].head(10), importance_censored['importance'].head(10))\n",
    "                    plt.xlabel('Feature Importance')\n",
    "                    plt.title('Top 10 Features (Censored=0)')\n",
    "                    plt.gca().invert_yaxis()\n",
    "                    plt.tight_layout()\n",
    "                    importance_path = os.path.join(output_dir, 'feature_importance_censored.png')\n",
    "                    plt.savefig(importance_path)\n",
    "                    plt.close()\n",
    "                    print(f\"Saved feature importance plot to {importance_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving plots: {e}\")\n",
    "        \n",
    "        return {\n",
    "            'rmse_censored': rmse_censored,\n",
    "            'mape_censored': mape_censored,\n",
    "            'feature_importance_censored': importance_censored,\n",
    "            'y_test_censored': y_test_c,\n",
    "            'y_pred_censored': y_pred_c,\n",
    "            'rmse_all': rmse_all,\n",
    "            'mape_all': mape_all,\n",
    "            'feature_importance_all': importance_all,\n",
    "            'y_test_all': y_test_a,\n",
    "            'y_pred_all': y_pred_a\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_random_forest_model: {e}\")\n",
    "        return None\n",
    "\n",
    "results = train_random_forest_model(df)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-manipulation-class",
   "language": "python",
   "name": "data-manipulation-class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
